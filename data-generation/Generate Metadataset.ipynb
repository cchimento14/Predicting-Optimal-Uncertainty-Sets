{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using DataFrames, Plots, StatsBase, JuMP, Statistics, Gurobi\n",
    "using DataFrames, CSV, LinearAlgebra, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to perform linear regression using an optimal uncertainty set; functions return the optimal $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function frobenius_one(X, y, ρ)\n",
    "    mod = Model(solver=GurobiSolver(OutputFlag=0, Quad=1, PSDTol=1e-3))\n",
    "    \n",
    "    n = size(X)[1]\n",
    "    p = size(X)[2]\n",
    "    \n",
    "    @variable(mod, beta[1:p])\n",
    "    @variable(mod, abs_beta[1:p])\n",
    "\n",
    "    @variable(mod, b)\n",
    "    @variable(mod, abs_Ter1[1:n]>=0)\n",
    "    \n",
    "    @constraint(mod, -beta .<= abs_beta)\n",
    "    @constraint(mod, beta .<= abs_beta)\n",
    "    \n",
    "    @constraint(mod, [i=1:p], abs_beta[i] <= b) # b = max of abs. value of elements of beta, which is the l-infinity norm\n",
    "    \n",
    "    @constraint(mod, -(y - X*beta) .<= abs_Ter1) # abs_Ter1 = abs(y - X*beta)\n",
    "    @constraint(mod, (y - X*beta) .<= abs_Ter1)\n",
    "    \n",
    "    @objective(mod, Min, sum(abs_Ter1) + ρ*b)\n",
    "    \n",
    "    solve(mod)\n",
    "    return(getvalue(beta))\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function frobenius_two(X, y, ρ)\n",
    "    mod = Model(solver=GurobiSolver(OutputFlag=0, Quad=1, PSDTol=1e-3))\n",
    "    \n",
    "    n = size(X)[1]\n",
    "    p = size(X)[2]\n",
    "    \n",
    "    @variable(mod, beta[1:p])\n",
    "    @variable(mod, Ter1>=0)\n",
    "    @variable(mod, Ter2>=0)\n",
    "    \n",
    "    @constraint(mod, Ter1 >= norm(y - X*beta)) # norm returns the l-2 norm\n",
    "    @constraint(mod, Ter2 >= ρ*norm(beta))\n",
    "    \n",
    "    @objective(mod, Min, Ter1 + Ter2)\n",
    "    \n",
    "    solve(mod)\n",
    "    return(getvalue(beta))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function frobenius_infinity(X, y, ρ)\n",
    "    mod = Model(solver=GurobiSolver(OutputFlag=0, Quad=1, PSDTol=1e-3))\n",
    "    \n",
    "    n = size(X)[1]\n",
    "    p = size(X)[2]\n",
    "    \n",
    "    @variable(mod, beta[1:p])\n",
    "    @variable(mod, abs_beta[1:p])\n",
    "    @variable(mod, max_Ter1)\n",
    "\n",
    "    @variable(mod, abs_Ter1[1:n]>=0)\n",
    "    \n",
    "    @constraint(mod, -beta .<= abs_beta) # abs_beta is the absolute value of the beta vector\n",
    "    @constraint(mod, beta .<= abs_beta)\n",
    "    \n",
    "    @constraint(mod, -(y - X*beta) .<= abs_Ter1) # abs_Ter1 = abs(y - X*beta)\n",
    "    @constraint(mod, (y - X*beta) .<= abs_Ter1)\n",
    "    \n",
    "    @constraint(mod, [i=1:n], abs_Ter1[i] <= max_Ter1) # max_Ter1 is the l-infinity norm of the vector (y-X*beta)\n",
    "    \n",
    "    @objective(mod, Min, max_Ter1 + ρ*sum(abs_beta))\n",
    "    \n",
    "    solve(mod)\n",
    "    return(getvalue(beta))\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function induced_one_two(X, y, ρ)\n",
    "    mod = Model(solver=GurobiSolver(OutputFlag=0, Quad=1, PSDTol=1e-3))\n",
    "    \n",
    "    n = size(X)[1]\n",
    "    p = size(X)[2]\n",
    "    \n",
    "    @variable(mod, beta[1:p])\n",
    "    @variable(mod, abs_Ter1[1:n]>=0)\n",
    "    @variable(mod, Ter2>=0)\n",
    "    \n",
    "    @constraint(mod, -(y - X*beta) .<= abs_Ter1) # abs_Ter1 = abs(y - X*beta)\n",
    "    @constraint(mod, (y - X*beta) .<= abs_Ter1)\n",
    "    \n",
    "    @constraint(mod, Ter2 >= ρ*norm(beta)) # Ter2 is ρ times the l-2 norm of beta\n",
    "    \n",
    "    @objective(mod, Min, sum(abs_Ter1) + Ter2)\n",
    "    \n",
    "    solve(mod)\n",
    "    return(getvalue(beta)) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function induced_two_one(X, y, ρ)\n",
    "    mod = Model(solver=GurobiSolver(OutputFlag=0, Quad=1, PSDTol=1e-3))\n",
    "    \n",
    "    n = size(X)[1]\n",
    "    p = size(X)[2]\n",
    "    \n",
    "    @variable(mod, beta[1:p])\n",
    "    @variable(mod, Ter1>=0)\n",
    "    @variable(mod, abs_beta[1:p])\n",
    "    \n",
    "    @constraint(mod, Ter1 >= norm(y - X*beta)) # Ter1 is the l-2 norm of (y - X*beta)\n",
    "    \n",
    "    @constraint(mod, -beta .<= abs_beta)\n",
    "    @constraint(mod, beta .<= abs_beta)\n",
    "    \n",
    "    @objective(mod, Min, Ter1 + ρ*sum(abs_beta))\n",
    "    \n",
    "    solve(mod)\n",
    "    return(getvalue(beta))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function induced_two_infinity(X, y, ρ)\n",
    "    mod = Model(solver=GurobiSolver(OutputFlag=0, Quad=1, PSDTol=1e-3))\n",
    "    \n",
    "    n = size(X)[1]\n",
    "    p = size(X)[2]\n",
    "    \n",
    "    @variable(mod, beta[1:p])\n",
    "    @variable(mod, Ter1>=0)\n",
    "    @variable(mod, abs_beta[1:p])\n",
    "    @variable(mod, b)\n",
    "    \n",
    "    @constraint(mod, Ter1 >= norm(y - X*beta)) # Ter1 is the l-2 norm of (y - X*beta)\n",
    "    \n",
    "    @constraint(mod, -beta .<= abs_beta)\n",
    "    @constraint(mod, beta .<= abs_beta)\n",
    "    \n",
    "    @constraint(mod, [i=1:p], abs_beta[i] <= b) # b = max of abs. value of elements of beta, which is the l-infinity norm\n",
    "    \n",
    "    @objective(mod, Min, Ter1 + ρ*b)\n",
    "    \n",
    "    solve(mod)\n",
    "    return(getvalue(beta))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function induced_infinity_two(X, y, ρ)\n",
    "    mod = Model(solver=GurobiSolver(OutputFlag=0, Quad=1, PSDTol=1e-3))\n",
    "    \n",
    "    n = size(X)[1]\n",
    "    p = size(X)[2]\n",
    "    \n",
    "    @variable(mod, beta[1:p])\n",
    "    @variable(mod, max_Ter1)\n",
    "    @variable(mod, abs_Ter1[1:n]>=0)\n",
    "    @variable(mod, Ter2>=0)\n",
    "    \n",
    "    @constraint(mod, -(y - X*beta) .<= abs_Ter1) # abs_Ter1 = abs(y - X*beta)\n",
    "    @constraint(mod, (y - X*beta) .<= abs_Ter1)\n",
    "    \n",
    "    @constraint(mod, [i=1:n], abs_Ter1[i] <= max_Ter1) # max_Ter1 is l-infinity norm of (y - X*beta)\n",
    "    \n",
    "    @constraint(mod, Ter2 >= ρ*norm(beta)) # Ter2 is ρ times the l-2 norm of beta\n",
    "    \n",
    "    @objective(mod, Min, max_Ter1 + Ter2)\n",
    "    \n",
    "    solve(mod)\n",
    "    return(getvalue(beta))\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to split a dataset into training, validation, and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function splitter(X_data, Y_data, percent_train, percent_valid)\n",
    "    # Concatenate X and Y data\n",
    "    data = hcat(Y_data, X_data)\n",
    "    \n",
    "    # Shuffle rows of X and Y data combined\n",
    "    data = data[shuffle(1:end), :]\n",
    "    \n",
    "    # Split X and Y data\n",
    "    Y_shuffled = data[:, end]\n",
    "    X_shuffled = data[:, 1:end-1]\n",
    "    \n",
    "    # Get indices for splitting your data\n",
    "    n = size(data)[1]\n",
    "    train_ind = Int(round(percent_train*n))\n",
    "    valid_ind = train_ind + Int(round(percent_valid*n))\n",
    "    test_ind = n\n",
    "    \n",
    "    # Split X data based on indices\n",
    "    X_train_data = X_shuffled[1:train_ind, :]\n",
    "    X_valid_data = X_shuffled[train_ind+1:valid_ind, :]\n",
    "    X_test_data = X_shuffled[valid_ind+1:n, :]\n",
    "    \n",
    "    # Split Y data based on indices\n",
    "    Y_train_data = Y_shuffled[1:train_ind, :]\n",
    "    Y_valid_data = Y_shuffled[train_ind+1:valid_ind, :]\n",
    "    Y_test_data = Y_shuffled[valid_ind+1:n, :]\n",
    "    \n",
    "    #Now we normalize features!\n",
    "    \n",
    "    #Center features\n",
    "    for i in 1:size(X_train_data,2)  \n",
    "        X_train_data[:,i] = X_train_data[:,i] .- mean(X_train_data[:,i]);\n",
    "        X_valid_data[:,i] = X_valid_data[:,i] .- mean(X_train_data[:,i]);\n",
    "        X_test_data[:,i] = X_test_data[:,i] .- mean(X_train_data[:,i]);\n",
    "    end\n",
    "    \n",
    "    #Calculate the std for each feature\n",
    "    num_dims = size(X_train_data,2)\n",
    "    std_feat = zeros(num_dims)\n",
    "    for i in 1:num_dims\n",
    "      std_feat[i] = std(X_train_data[:,i]);\n",
    "    end\n",
    "    \n",
    "    #scale the features\n",
    "    for i in 1:num_dims\n",
    "        X_train_data[:,i]=X_train_data[:,i] ./std_feat[i]\n",
    "        X_valid_data[:,i] = X_valid_data[:,i] ./ std_feat[i];\n",
    "        X_test_data[:,i] = X_test_data[:,i] ./ std_feat[i];\n",
    "    end\n",
    "\n",
    "    #Center Labels\n",
    "    mean_train_y = mean(Y_train_data);\n",
    "    Y_train_data =  Y_train_data .- mean_train_y;\n",
    "    Y_valid_data = Y_valid_data .- mean_train_y;\n",
    "    Y_test_data = Y_test_data .- mean_train_y;\n",
    "    \n",
    "     \n",
    "    return(X_train_data, X_valid_data, X_test_data, Y_train_data, Y_valid_data, Y_test_data)\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sample dataset so we can test the functions\n",
    "X_train, X_valid, X_test, Y_train, Y_valid, Y_test = splitter(X_matrix, Y_matrix, 0.5, 0.25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE function to validate ρ values and evaluate uncertainty sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function MAE(X_mat, y_vec, beta_vec)\n",
    "    return(sum(broadcast(abs, y_vec - X_mat*beta_vec)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best value of ρ for a robust optimization problem through validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function row_tester(row_list, X_train, Y_train, X_valid, Y_valid, func)\n",
    "    \n",
    "    MAE_errors = []\n",
    "    \n",
    "    # Loop through different possible values for ρ\n",
    "    # Calculate beta value (training data) and then MAE value (validation data) for that particular ρ\n",
    "    # Save your MAE values in an array\n",
    "    for p in row_list\n",
    "        beta_temp = func(X_train, Y_train, p)\n",
    "        MAE_tmp = MAE(X_valid, Y_valid, beta_temp)\n",
    "        append!(MAE_errors, MAE_tmp)\n",
    "    end\n",
    "    \n",
    "    min_index = argmin(MAE_errors)  # find the minimum MAE  \n",
    "    return(row_list[min_index]) # return the value of ρ that gave the minimum MAE on the validation set\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test all uncertainty sets on a dataset and return the name of the one that resulted in the lowest test MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function best_uncertainty_set(X_train, X_valid, X_test, Y_train, Y_valid, Y_test, row_range)\n",
    "    # Set up dictionary with starting values (to be replaced)\n",
    "    sets_dict = Dict([(\"Frobenius One\", 0.5), (\"Frobenius Two\", 0.5), (\"Frobenius Infinity\", 0.5),\n",
    "                  (\"Induced One Two\", 0.5), (\"Induced Two One\", 0.5), \n",
    "                    (\"Induced Two Infinity\", 0.5), (\"Induced Infinity Two\", 0.5)])\n",
    "    \n",
    "    # Find best ρ for Frobenius One uncertainty set, and save MAE for Frobenius One beta\n",
    "    row_frob_one = row_tester(row_range, X_train, Y_train, X_valid, Y_valid, frobenius_one)\n",
    "    beta_frob_one = frobenius_one(X_train, Y_train, row_frob_one)\n",
    "    sets_dict[\"Frobenius One\"] = MAE(X_test, Y_test, beta_frob_one)\n",
    "    \n",
    "    # Find best ρ for Frobenius Two uncertainty set, and save MAE for Frobenius Two beta\n",
    "    row_frob_two = row_tester(row_range, X_train, Y_train, X_valid, Y_valid, frobenius_two)\n",
    "    beta_frob_two = frobenius_two(X_train, Y_train, row_frob_one)\n",
    "    sets_dict[\"Frobenius Two\"] = MAE(X_test, Y_test, beta_frob_two)\n",
    "    \n",
    "    # Find best ρ for Frobenius Infinity uncertainty set, and save MAE for Frobenius Infinity beta\n",
    "    row_frob_infinity = row_tester(row_range, X_train, Y_train, X_valid, Y_valid, frobenius_infinity)\n",
    "    beta_frob_infinity = frobenius_infinity(X_train, Y_train, row_frob_infinity)\n",
    "    sets_dict[\"Frobenius Infinity\"] = MAE(X_test, Y_test, beta_frob_infinity)\n",
    "    \n",
    "    # Find best ρ for Induced One Two uncertainty set, and save MAE for Induced One Two beta\n",
    "    row_induced_12 = row_tester(row_range, X_train, Y_train, X_valid, Y_valid, induced_one_two)\n",
    "    beta_induced_12 = induced_one_two(X_train, Y_train, row_induced_12)\n",
    "    sets_dict[\"Induced One Two\"] = MAE(X_test, Y_test, beta_induced_12)\n",
    "    \n",
    "    # Find best ρ for Induced Two One uncertainty set, and save MAE for Induced Two One beta\n",
    "    row_induced_21 = row_tester(row_range, X_train, Y_train, X_valid, Y_valid, induced_two_one)\n",
    "    beta_induced_21 = induced_two_one(X_train, Y_train, row_induced_21)\n",
    "    sets_dict[\"Induced Two One\"] = MAE(X_test, Y_test, beta_induced_21)\n",
    "    \n",
    "    # Find best ρ for Induced Two Infinity uncertainty set, and save MAE for Induced Two Infinity beta\n",
    "    row_induced_2inf = row_tester(row_range, X_train, Y_train, X_valid, Y_valid, induced_two_infinity)\n",
    "    beta_induced_2inf = induced_two_infinity(X_train, Y_train, row_induced_2inf)\n",
    "    sets_dict[\"Induced Two Infinity\"] = MAE(X_test, Y_test, beta_induced_2inf)\n",
    "    \n",
    "    # Find best ρ for Induced Infinity Two uncertainty set, and save MAE for Induced Infinity Two beta\n",
    "    row_induced_inf2 = row_tester(row_range, X_train, Y_train, X_valid, Y_valid, induced_infinity_two)\n",
    "    beta_induced_inf2 = induced_infinity_two(X_train, Y_train, row_induced_inf2)\n",
    "    sets_dict[\"Induced Infinity Two\"] = MAE(X_test, Y_test, beta_induced_inf2)\n",
    "    \n",
    "    # Return name of uncertainty set that had t\n",
    "    return(findmin(sets_dict)[2])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test only the most common uncertainty sets on a dataset and return the name of the one that resulted in the lowest test MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function best_uncertainty_reduced(X_train, X_valid, X_test, Y_train, Y_valid, Y_test, row_range)\n",
    "    # Set up dictionary with starting values (to be replaced)\n",
    "    sets_dict = Dict([(\"Induced Two One\", 0.5), \n",
    "                    (\"Induced Two Infinity\", 0.5), \n",
    "            (\"Induced Infinity Two\", 0.5)])\n",
    "     \n",
    "    # Find best ρ for Induced Two One uncertainty set, and save MAE for Induced Two One beta\n",
    "    row_induced_21 = row_tester(row_range, X_train, Y_train, X_valid, Y_valid, induced_two_one)\n",
    "    beta_induced_21 = induced_two_one(X_train, Y_train, row_induced_21)\n",
    "    sets_dict[\"Induced Two One\"] = MAE(X_test, Y_test, beta_induced_21)\n",
    "    \n",
    "    # Find best ρ for Induced Two Infinity uncertainty set, and save MAE for Induced Two Infinity beta\n",
    "    row_induced_2inf = row_tester(row_range, X_train, Y_train, X_valid, Y_valid, induced_two_infinity)\n",
    "    beta_induced_2inf = induced_two_infinity(X_train, Y_train, row_induced_2inf)\n",
    "    sets_dict[\"Induced Two Infinity\"] = MAE(X_test, Y_test, beta_induced_2inf)\n",
    "    \n",
    "    # Find best ρ for Induced Infinity Two uncertainty set, and save MAE for Induced Infinity Two beta\n",
    "    row_induced_inf2 = row_tester(row_range, X_train, Y_train, X_valid, Y_valid, induced_infinity_two)\n",
    "    beta_induced_inf2 = induced_infinity_two(X_train, Y_train, row_induced_inf2)\n",
    "    sets_dict[\"Induced Infinity Two\"] = MAE(X_test, Y_test, beta_induced_inf2)\n",
    "    \n",
    "    # Return name of uncertainty set that had t\n",
    "    return(findmin(sets_dict)[2])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a function that returns features for the Optimal Classification tree to split upon\n",
    "#### Features: $R^2$, domain, number of features, number of observations, normalized variance of the labels, avg normalzied variance of the features, median normalized variance of the features, range of the normalized variance of the features, percentage of outliers in the labels, percent of features that have at least some threshold of correlation, percent of missing labels, avg percent of missing values in the features, mean correlation (fisher's z-test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty features dataframe\n",
    "meta_df = DataFrame(\n",
    "    Dataset_Name = String[], \n",
    "    Source = String[],\n",
    "    y_miss_per = Float64[],\n",
    "    avg_per_miss_x = Float64[],\n",
    "    r2_rr = Float64[], \n",
    "    n = Int64[], \n",
    "    p = Int64[],\n",
    "    ind_Disp_y = Float64[], \n",
    "    x_mean_norm_var = Float64[],\n",
    "    median_x_norm_var = Float64[],\n",
    "    range_norm_x_var = Float64[], \n",
    "    y_outliers_per = Float64[],\n",
    "    x_corr_per = Float64[], \n",
    "    mean_cor = Float64[],\n",
    "    uncertainty_set = String[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of filtered R Datasets\n",
    "### We filtered out certain R packages and datasets with fewer than 40 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDataset_List = CSV.read(\"RDatasets_Filtered.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through the R datasets, generate features, and add optimal uncertainty set label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to impute missing data with k-nearest-neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function knn_impute(dataset)\n",
    "    # Convert NaN to missing\n",
    "    for col in 1:size(dataset, 2)\n",
    "        replace(dataset[!,col], NaN => missing)\n",
    "    end\n",
    "    \n",
    "    # Cross validation\n",
    "    grid = IAI.GridSearch(IAI.ImputationLearner(random_seed=2),\n",
    "        (method=:opt_knn, knn_k = [5:5:100;]))\n",
    "    best_mod = IAI.get_learner(grid)\n",
    "\n",
    "    IAI.fit!(best_mod,dataset)\n",
    "    data_imputed = IAI.transform(best_mod, dataset)\n",
    "    return(data_imputed)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to split a dataset into X and y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function features(data)\n",
    "    data = convert(Matrix,data)\n",
    "    X = data[:,1:end-1]\n",
    "    y = data[:,end]\n",
    "    \n",
    "    # split data\n",
    "    X_train, X_valid, X_test, Y_train, Y_valid, Y_test = splitter(X,y,0.5,0.25)\n",
    "    \n",
    "    r2_rr = R2_RR(X_train, X_valid, X_test, Y_train, Y_valid, Y_test) ##returns r^2 for ridge regression\n",
    "    \n",
    "    n = size(X,1)\n",
    "    p = size(X,2)\n",
    "    \n",
    "    if mean(y) == 0 && var(y) == 0\n",
    "        ind_Disp_y = 0\n",
    "    else\n",
    "        ind_Disp_y = var(y)/mean(y)\n",
    "    end\n",
    "    \n",
    "    x_mean_norm_var, x_norm_var = mean_norm_var(X)\n",
    "    \n",
    "    #mean normalized variance of the features\n",
    "    median_x_norm_var = median(x_norm_var)\n",
    "\n",
    "    range_norm_x_var = maximum(x_norm_var)-minimum(x_norm_var)\n",
    "    \n",
    "    y_outliers_per = count(i-> -2*std(y)<i<2*std(y), y)/length(y)\n",
    "    \n",
    "    #mean_cor = mean(cor(X)) this is wrong!\n",
    "    #percentage of features that are correlated above a certain threshold\n",
    "    x_corr_per = 1- count(i-> (-0.4<i<0.4),cor(X))/(length(cor(X))-p) \n",
    "    \n",
    "    ## vectorize correlation matrix. z transform each element, then average, then reverse transform. \n",
    "    ## This approach requires some assumptions on the data\n",
    "    mean_cor = correlation_metric(X,p)\n",
    "    \n",
    "    uncertainty_set = best_uncertainty_reduced(X_train, X_valid, X_test, Y_train, Y_valid, Y_test, collect(0:0.5:10))\n",
    "     \n",
    "    return [r2_rr, n, p, \n",
    "        ind_Disp_y, x_mean_norm_var, median_x_norm_var,\n",
    "        range_norm_x_var, y_outliers_per, x_corr_per, mean_cor, uncertainty_set]      \n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function perc_missing_x(X)\n",
    "    x_miss = []\n",
    "    for j in 1:size(X,2)\n",
    "        append!(x_miss, count(i -> typeof(i)==Missing, X[:,j])/length(X[:,j]))\n",
    "    end\n",
    "    return mean(x_miss)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function perc_missing_y(Y)\n",
    "    return (count(i -> typeof(i)==Missing, Y[:,1])/size(Y, 1))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mean_norm_var(X)\n",
    "    x_norm_var = []\n",
    "    for i in 1:size(X,2)\n",
    "        if mean(X[:,i]) == 0\n",
    "            append!(x_norm_var, 0)\n",
    "        else\n",
    "            append!(x_norm_var, var(X[:,i])/mean(X[:,i]))\n",
    "        end\n",
    "    end\n",
    "    return mean(x_norm_var), x_norm_var #mean normalized variance of the features\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function correlation_metric(X,p)\n",
    "    vec_cor =[]\n",
    "    mat_cor = cor(X)\n",
    "    for i in 1:p\n",
    "        for j in 1:p\n",
    "            if j>i\n",
    "                append!(vec_cor,mat_cor[i,j])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return tanh(mean(atanh.(vec_cor)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression for calculating the β's to find an estimate $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function L2(train_X,train_y,ρ)\n",
    "    m = Model(solver=GurobiSolver(PSDTol=1e-3))\n",
    "    \n",
    "    P = size(train_X)[2]\n",
    "    n = size(train_X)[1]\n",
    "    \n",
    "    @variable(m, z[1:2])\n",
    "    @variable(m, β[1:P])\n",
    "\n",
    "\n",
    "    @constraint(m, z[1] >= sum((train_y[i] - transpose(β)*train_X[i,:])^2 for i in 1:n))\n",
    "    @constraint(m, z[2] >= sum(β[i]^2 for i in 1:P))\n",
    "\n",
    "    \n",
    "    @objective(m, Min, z[1] + ρ*z[2])\n",
    "    \n",
    "    solve(m)\n",
    "    \n",
    "    return getvalue(β)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaption of earlier function that uses MSE instead to judge the validation loss because ridge regression optimimizes on squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function row_tester_MSE(row_list, X_train, Y_train, X_valid, Y_valid, func)\n",
    "    \n",
    "    MSE_errors = []\n",
    "    βs = []\n",
    "    \n",
    "    # Loop through different possible values for ρ\n",
    "    # Calculate beta value (training data) and then MAE value (validation data) for that particular ρ\n",
    "    # Save your MSE values in an array\n",
    "    for p in row_list\n",
    "        beta_temp = func(X_train, Y_train, p)\n",
    "        push!(βs,beta_temp)\n",
    "        MSE_tmp = MSE(X_valid, Y_valid, beta_temp)\n",
    "        append!(MSE_errors, MSE_tmp)\n",
    "    end\n",
    "    \n",
    "    min_index = argmin(MSE_errors)  # find the index of that minimum MSE\n",
    "    return(βs[min_index]) # return the value of β that gave the minimum MSE on the validation set\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function MSE(X_valid,Y_valid,beta_temp)\n",
    "    return sum((Y_valid[i] - transpose(beta_temp)*X_valid[i,:])^2 for i=1:size(X_valid,1))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function R2_RR(X_train, X_valid, X_test, Y_train, Y_valid, Y_test) #returns a different value for R^2 because of randomization of the splits\n",
    "    #We want to use all the data, because we only care about finding ρ to generate an unbiased $R^2$\n",
    "    opt_β = row_tester_MSE([0.1,1,5,10,20],X_train,Y_train,X_valid,Y_valid,L2);\n",
    "    y_hat = X_test*opt_β\n",
    "    mean_y = mean(Y_test)\n",
    "    y_bar = fill(mean_y,size(Y_test,1))\n",
    "    return 1 - sum((Y_test .- y_hat).^2)/sum((Y_test .- y_bar).^2)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are going to use $R^2$ as a feature, we must understand what it is doing so that we can interpret the OCT results. One possible interpretation is that $R^2$ guages how amenable a dataset is to a linear model. But to calculate, we need estimates, $\\hat{y}$, that come from β. Estimating the β requires some approach, and statistical learning theory shows us that different approaches have different biases. Since our  metric for evaluating performance on the regressions is Ordinary Least Squares, $||Y-Xβ||^2$ is the natural loss function to minimize to recover the β. If the system is overdetermined, (n>d), there is not guaranteed to exist a β that satisfies $Y = Xβ$. The normal equations yield the β that minimizes squared loss-- $β=(X^TX)^{-1}X^TY$. But if $d >>n $, then taking the inverse of $X^TX$ becomes less computationally feasible. If the system is underdetermined, n<d, the solution is possibly not unique. \n",
    "\n",
    "One approach in calculating $R^2$ for the purposes of classifying the datasets is to use 1) $β=(X^TX)^{-1}X^T$ if n>d, and 2) $β=X^T(XX^T)^{-1}$ if n<d. The tradeoff is that while 1) minimizes squared loss, 2) minimizes the squared sum of the components of β s.t. $Y=Xβ$. So the predictor would be arbitrarily biased depending on whether n>d or n<d.\n",
    "\n",
    "Another approach to calculate $R^2$ for the purposes of classifying the datasets is to use Gurobi to solve ridge regression, because we know that this scales for large datasets. We can then interpret the Optimal Classification Tree results. We use the latter for the feature included in Optimal Classification Tree. We provide a range for ρ and select the best via cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:size(RDataset_List, 1)\n",
    "    name = RDataset_List[i,:Dataset] # name of dataset to read in\n",
    "    source = RDataset_List[i,:Package] # source of dataset to read in\n",
    "    print(name, \" \", source, \"\\n\")\n",
    "    tmp_df = dataset(source, name) # read in datasets one at a time\n",
    "    \n",
    "    \n",
    "    tmp_features = Any[name, source] # Create a list of features to add to meta_df\n",
    "    \n",
    "    # Find missing data points\n",
    "    y_miss_per = perc_missing_y(tmp_df[!,end])\n",
    "    avg_per_miss_x = perc_missing_x(convert(Matrix, tmp_df[!,1:end-1]))\n",
    "    append!(tmp_features, y_miss_per)\n",
    "    append!(tmp_features, avg_per_miss_x)\n",
    "    \n",
    "    # Get a list of the column types in tmp_df\n",
    "    col_types = []\n",
    "    for n in names(tmp_df)\n",
    "        push!(col_types, typeof(tmp_df[!,n]))     \n",
    "    end\n",
    "    \n",
    "    # Skip datasets with non-numeric values\n",
    "    non_numeric = false\n",
    "    for t in col_types\n",
    "        if occursin(\"Categorical\", string(t))\n",
    "            non_numeric = true\n",
    "        elseif occursin(\"String\", string(t))\n",
    "            non_numeric = true\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if non_numeric == true\n",
    "        continue\n",
    "    end\n",
    "    \n",
    "    tmp_df = knn_impute(tmp_df) # Impute any missing data  \n",
    "    \n",
    "    print(name, \" \", source, \" \" , i, \"\\n\")\n",
    "        \n",
    "    features(tmp_df)\n",
    "    append!(tmp_features, features(tmp_df))\n",
    "    \n",
    "    push!(meta_df, tmp_features)     \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save metadata as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"Final_Metadata_Reduced.csv\", meta_df);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
